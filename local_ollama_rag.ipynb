{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d5cab6-515e-4dd6-95ae-6393f0c4435c",
   "metadata": {},
   "source": [
    "## Ingesting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e8b999-83ba-484a-9b94-f56c201d2036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q unstructured langchain langchain-community\n",
    "%pip install --q \"unstructured[all-docs]\" ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jahan\\AppData\\Local\\Temp\\ipykernel_1360\\3632673901.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from IPython.display import display as Markdown\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104c0b18-1c06-41a1-a2ca-f9ee23f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"testfile.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38119195-9c91-4e58-aa46-8a74244032af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life\\n\\nREVISED: MICHAEL LOLKUS 2018\\n\\nTables, Charts, and Graphs Basics\\n\\n\\uf075 We use charts and graphs to visualize data.\\n\\n\\uf075 This data can either be generated data, data gathered from\\n\\nan experiment, or data collected from some source.\\n\\n\\uf075 A picture tells a thousand words so it is not a surprise that\\n\\nmany people use charts and graphs when explaining data.\\n\\nTypes of Visual Representations of Data\\n\\nTable of Yearly U.S. GDP by Industry (in millions of dollars)\\n\\nSource: U.S. Bureau of Labor Statistics\\n\\nYear\\n\\n2010\\n\\n2011\\n\\n2012\\n\\n2013\\n\\nAll Industries\\n\\n26093515\\n\\n27535971\\n\\n28663246\\n\\n29601191\\n\\nManufacturing\\n\\n4992521\\n\\n5581942\\n\\n5841608\\n\\n5953299\\n\\nFinance, Insurance, Real Estate, Rental, Leasing\\n\\n4522451\\n\\n4618678\\n\\n4797313\\n\\n5031881\\n\\nArts, Entertainment, Recreation, Accommodation, and Food Service\\n\\n964032\\n\\n1015238\\n\\n1076249\\n\\n1120496\\n\\nOther\\n\\n15614511\\n\\n16320113\\n\\n16948076\\n\\n17495515\\n\\n2014\\n\\n30895407\\n\\n6047477\\n\\n5339678\\n\\n1189646\\n\\n18318606\\n\\n2015\\n\\n31397023\\n\\n5829554\\n\\n5597018\\n\\n1283813\\n\\n18686638\\n\\nThe chart below is called a pie chart. It shows what percent “of the pie” a particular category occupies out of the whole. If total GDP in 2015 is the entire pie, then manufacturing makes up 19% of that pie and finance makes up 18%. Notice that visually speaking, since 19% and 18% are so close to each other in value, their respective slices of the pie are similarly sized. Manufacturing\\n\\nThe chart below is called a pie chart. It shows what percent “of the pie” a particular category occupies out of the whole. If total GDP in 2015 is the entire pie, then manufacturing makes up 19% of that pie and finance makes up 18%. Notice that visually speaking, since 19% and 18% are so close to each other in value, their respective slices of the pie are similarly sized. 2015 U.S. GDP (in millions of dollars)\\n\\n\\n\\n19%\\n\\nFinance, insurance, real estate, rental, and leasing\\n\\n59%\\n\\n18%\\n\\nArts, entertainment, recreation, accommodation, and food services\\n\\nOther\\n\\n\\uf075 Pie charts can be misleading when the slices do not\\n\\ncorrespond with the percent contribution to the whole pie.\\n\\n\\uf075 Notice the pie chart below is not very intuitive.\\n\\nExample from Everyday Life\\n\\nThe following chart shows how a family spends its yearly income of $31,000. How much money does this family spend on transportation?\\n\\nFamily Budget of $31,000\\n\\n19%\\n\\n25%\\n\\n10%\\n\\n15%\\n\\n26%\\n\\n5%\\n\\nOther\\n\\nRecreation\\n\\nTransportation\\n\\nClothing\\n\\nhousing\\n\\nFood\\n\\nSolution\\n\\n\\uf075 The chart indicates that 15% of the income is spent on transportation. We must answer the question: 15% of $31,000 is what?\\n\\n\\uf075 Writing as an equation and solving, we get\\n\\n\\uf075 n = 0.15 x 31,000 = 4650\\n\\n\\uf075 So the family spends $4650 on transportation yearly.\\n\\nThe graph below is called a bar graph. •\\n\\nIt shows each of the variables independent of each other, each with its own bar.\\n\\n2015 GDP for all industries was $31.397023; looking at the graph, the bar for all industries is just above $30.\\n\\nOne is still be able compare each variable with the other by comparing bars.\\n\\n2015 GDP (in trillions of dollars)\\n\\nOther\\n\\nArts, entertainment, recreation, accommodation, and food services\\n\\nFinance, insurance, real estate, rental, and leasing\\n\\nManufacturing\\n\\nAll industries\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\nDollars\\n\\n30\\n\\n35\\n\\n\\n\\nThe graph below is called a line graph. It shows how a variable evolves with respect to another variable. In the line graph below, we show how GDP has evolved by year.\\n\\nYearly Total GDP (in trillions of dollars)\\n\\n35\\n\\n30\\n\\n25\\n\\ns r a\\n\\n20\\n\\nl l\\n\\no D\\n\\n15\\n\\nYearly Total GDP\\n\\n10\\n\\n5\\n\\n0\\n\\n7 4 9 1\\n\\n0 5 9 1\\n\\n3 5 9 1\\n\\n6 5 9 1\\n\\n9 5 9 1\\n\\n2 6 9 1\\n\\n5 6 9 1\\n\\n8 6 9 1\\n\\n1 7 9 1\\n\\n4 7 9 1\\n\\n7 7 9 1\\n\\n0 8 9 1\\n\\n3 8 9 1\\n\\n6 8 9 1\\n\\n9 8 9 1\\n\\n2 9 9 1\\n\\n5 9 9 1\\n\\n8 9 9 1\\n\\n1 0 0 2\\n\\n4 0 0 2\\n\\n7 0 0 2\\n\\n0 1 0 2\\n\\n3 1 0 2\\n\\nYear\\n\\nWhen to use a Line Graph, Pie Chart, or Bar Graph? \\uf075 We use the pie chart here to compare parts of a whole. In our example, we compared components of US GDP.\\n\\n\\uf075 The line chart is useful when you want to show how a\\n\\nvariable changes over time. For our purposes, we used it show how GDP changed over time.\\n\\n\\uf075 Bar graphs are good for comparing different groups of\\n\\nvariables. We used it to compare different components of US GDP. We did the same with the pie chart; depending on your purposes you may choose to use a pie chart or a bar graph.\\n\\n\\n\\nIf given a table of data, we should be able to plot it. Below is some sample data; plot the data with x on the x-axis and y on the y-axis.\\n\\nx\\n\\ny\\n\\n0\\n\\n0\\n\\n1\\n\\n3\\n\\n2\\n\\n6\\n\\n3\\n\\n9\\n\\n4\\n\\n12\\n\\n5\\n\\n15\\n\\n6\\n\\n18\\n\\n7\\n\\n21\\n\\n8\\n\\n24\\n\\nBelow is a plot of the data on the table from the previous slide. Notice that this plot is a straight line meaning that a linear equation must have generated this data.\\n\\nWhat if the data is not generated by a linear equation? We can fit the data using a linear regression and use that line as an approximation to the data. Regressions are beyond the scope of this workshop.\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\n0\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\nExample from Urban Affairs\\n\\n\\uf075 What kind of bar graph is this?\\n\\n\\uf075 Whose life expectancy has changed the most since 1925?\\n\\n\\uf075 In 1925, about how much longer was a woman expected\\n\\nto live than a man?\\n\\nExample from History\\n\\nIn what years were the affiliations for Republicans and Independents the same? During what time period did the party affiliations have the most change?\\n\\nExample from Education\\n\\nWhat percent of the total class received grades of 72 or 77?\\n\\nWhich grade showed the largest difference between males and females?\\n\\nExample from Psychology\\n\\nWhat do you notice is different in this graph than the others reviewed so far?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview first page\n",
    "Markdown(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d5435cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED     \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    15 hours ago    \n",
      "llama2:latest              78e26419b446    3.8 GB    15 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dcf2cfe-a7aa-4ecf-85e3-f77b9e850514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Pull nomic-embed-text model from Ollama if you don't have it\n",
    "# !ollama pull nomic-embed-text\n",
    "# # List models again to confirm it's available\n",
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014e862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: chromadb 0.4.22\n",
      "Uninstalling chromadb-0.4.22:\n",
      "  Successfully uninstalled chromadb-0.4.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: protobuf 5.29.1\n",
      "Uninstalling protobuf-5.29.1:\n",
      "  Successfully uninstalled protobuf-5.29.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jahan\\anaconda3\\Lib\\site-packages\\google\\~.pb'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.68.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.29.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. First clean up any existing ChromaDB installations\n",
    "%pip uninstall -y chromadb\n",
    "%pip uninstall -y protobuf\n",
    "\n",
    "# 2. Install specific versions known to work together\n",
    "%pip install -q protobuf==3.20.3\n",
    "%pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "%pip install -q langchain-ollama\n",
    "\n",
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18dd152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.4.22\n",
      "  Using cached chromadb-0.4.22-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.2.2.post1)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (2.32.3)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (3.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.29.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from chromadb==0.4.22) (5.0.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.22) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.22) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.22) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.22) (0.41.3)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.22) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (3.20.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.22) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.22) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.22) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.22) (1.29.0)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb==0.4.22)\n",
      "  Using cached protobuf-5.29.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.22) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.22) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.22) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.22) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.22) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb==0.4.22) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb==0.4.22) (3.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb==0.4.22) (0.27.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.22) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.22) (14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.22) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.22) (3.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from starlette<0.42.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22) (4.7.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.22) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.22) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.22) (1.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.22) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jahan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.22) (0.4.8)\n",
      "Using cached chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
      "Using cached protobuf-5.29.1-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: protobuf, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed chromadb-0.4.22 protobuf-5.29.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb==0.4.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ed9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 5.29.1\n",
      "Uninstalling protobuf-5.29.1:\n",
      "  Successfully uninstalled protobuf-5.29.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.68.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.29.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. First clean up any existing ChromaDB installations\n",
    "#%pip uninstall -y chromadb\n",
    "%pip uninstall -y protobuf\n",
    "\n",
    "# 2. Install specific versions known to work together\n",
    "%pip install -q protobuf==3.20.3\n",
    "#%pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "%pip install -q langchain-ollama\n",
    "\n",
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# 4. Now reimport with the new versions\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 5. Try creating the vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3f6c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED     \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    15 hours ago    \n",
      "llama2:latest              78e26419b446    3.8 GB    15 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"llama2\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06c25c1d-d205-409e-90a2-179d0bd7c41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (8.4 GiB) than is available (2.9 GiB)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom page 6 get the tabular data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3726\u001b[0m         ]\n\u001b[1;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3726\u001b[0m         ]\n\u001b[1;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3711\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3709\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   3710\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m-> 3711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   3712\u001b[0m     step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   3713\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3714\u001b[0m     child_config,\n\u001b[0;32m   3715\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:266\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    265\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    269\u001b[0m         result,\n\u001b[0;32m    270\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:168\u001b[0m, in \u001b[0;36mMultiQueryRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    156\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    158\u001b[0m     run_manager: CallbackManagerForRetrieverRun,\n\u001b[0;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get relevant documents given a user query.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m        Unique union of relevant documents from all generated queries\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_queries(query, run_manager)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_original:\n\u001b[0;32m    170\u001b[0m         queries\u001b[38;5;241m.\u001b[39mappend(query)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:185\u001b[0m, in \u001b[0;36mMultiQueryRetriever.generate_queries\u001b[1;34m(self, question, run_manager)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_queries\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate queries based upon user input.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m        List of LLM generated queries that are similar to the user input\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    186\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question}, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain, LLMChain):\n\u001b[0;32m    189\u001b[0m         lines \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py:690\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    685\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    689\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 690\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    691\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[0;32m    694\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    695\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[0;32m    696\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m    701\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py:591\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    584\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    590\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    593\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[0;32m    594\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[0;32m    595\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m                 ),\n\u001b[0;32m    609\u001b[0m             )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_ollama\\chat_models.py:578\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:167\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    166\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 167\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m    170\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mResponseError\u001b[0m: model requires more system memory (8.4 GiB) than is available (2.9 GiB)"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"From page 6 get the tabular data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe79f21-48aa-4820-aa9f-79f3d1a0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
